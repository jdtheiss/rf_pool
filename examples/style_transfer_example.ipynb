{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from context import rf_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load VGG19 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg19 = torchvision.models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rf_pool.models.FeedForwardNetwork(vgg19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that weights were loaded by viewing first layer\n",
    "rf_pool.utils.visualize.show_weights(model, 'features', 'conv2d0_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Image Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set transform\n",
    "transform = transforms.Compose([transforms.Resize(224),\n",
    "                                transforms.CenterCrop((224,224)),\n",
    "                                transforms.ToTensor(),\n",
    "                                rf_pool.ops.Op(lambda x: x / torch.max(x)),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                     std=[0.229, 0.224, 0.225]),\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get urls for dataset\n",
    "base_url = 'https://random-ize.com/random-art-gallery/'\n",
    "# set url dataset\n",
    "styleset = rf_pool.utils.datasets.URLDataset('.', urls=[base_url], transform=transform,\n",
    "                                             find_img_url=True, url_pattern='src=\"(.+\\.jpg)\"',\n",
    "                                             url_replace=['/random-art-gallery/', base_url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get urls for dataset\n",
    "base_url = 'https://loremflickr.com/300/300'\n",
    "# set url dataset\n",
    "contentset = rf_pool.utils.datasets.URLDataset('.', urls=[base_url], transform=transform,\n",
    "                                               find_img_url=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Style Transfer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_loss_fn(target, seed):\n",
    "    t = torch.flatten(target, -2) \n",
    "    t = torch.matmul(t, t.transpose(-2,-1))\n",
    "    s = torch.flatten(seed, -2)\n",
    "    s = torch.matmul(s, s.transpose(-2,-1))\n",
    "    return torch.nn.MSELoss()(t, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Content and Style Images\n",
    "content_img = contentset[0][0].unsqueeze(0)\n",
    "plt.imshow(rf_pool.utils.functions.normalize_range(content_img[0]).permute(1,2,0))\n",
    "plt.show()\n",
    "\n",
    "style_img = styleset[0][0].unsqueeze(0)\n",
    "plt.imshow(rf_pool.utils.functions.normalize_range(style_img[0]).permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style Transfer with Content and Style Losses\n",
    "seed = content_img.detach().requires_grad_(True)\n",
    "\n",
    "content_loss = rf_pool.losses.LayerLoss(model, {'features': {'conv2d30': []}}, \n",
    "                                        torch.nn.MSELoss(), input_target=content_img)\n",
    "style_loss = rf_pool.losses.LayerLoss(model, {'features': dict([('conv2d%d' % d, []) \n",
    "                                                                for d in [0, 5, 10, 19, 28]])}, \n",
    "                                      gram_loss_fn, input_target=style_img)\n",
    "loss_fn = rf_pool.losses.MultiLoss(losses=[content_loss, style_loss], \n",
    "                                   weights=[1e-3, 1e-2])\n",
    "\n",
    "optim = torch.optim.Adam([seed], lr=5e-2)\n",
    "model.optimize_texture(1000, seed, loss_fn, optim, monitor=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
